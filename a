cryptos_list = ["BTCUSDT", "ETHUSDT", "BNBUSDT", "SOLUSDT", "XRPUSDT", "DOGEUSDT", "ADAUSDT", "SHIB1000USDT", "AVAXUSDT", "TRXUSDT"]
data_dic = {}
for crypto in cryptos_list:
    data_dic[crypto] = Datas(crypto, S, L, span, tgt_volitility, CPD=False)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

def Dataloader_X(datas, T, batch_size, train_days):
    names = list(datas.keys())
    days = datas["BTCUSDT"].U.values.shape[0]
    test_days = days - train_days - T
    N_data = []
    for name in names:
        N_data.append(np.array(list(datas[name].U.values), dtype=float))
    N_data_np = np.stack(N_data)
    N_T_data_train = np.array([N_data_np[:, i:i+T] for i in range(train_days)])
    N_T_data_test = np.array([N_data_np[:, i:i+T] for i in range(train_days, days-T+1)])
    A = [torch.tensor(N_T_data_train[i*batch_size:], dtype=torch.float).to(device) if i == train_days//batch_size else torch.tensor(N_T_data_train[i*batch_size:(i+1)*batch_size], dtype=torch.float).to(device) for i in range(train_days//batch_size+1)]
    B = [torch.tensor(N_T_data_test[i*batch_size:], dtype=torch.float).to(device) if i == test_days//batch_size else torch.tensor(N_T_data_test[i*batch_size:(i+1)*batch_size], dtype=torch.float).to(device) for i in range(test_days//batch_size+1)]
    print("Size of batched model_input U")
    print("Training set")
    print(f"num of batch : {len(A)}, first batch : {A[0].size()}, last_batch : {A[-1].size()}")
    print("Testing set")
    print(f"num of batch : {len(B)}, first batch : {B[0].size()}, last_batch : {B[-1].size()}")
    return A, B

def Dataloader_X_CPD(datas, T, batch_size, train_days):
    names = list(datas.keys())
    days = datas["BTCUSDT"].U_CPD.values.shape[0]
    test_days = days - train_days - T
    N_data = []
    for name in names:
        N_data.append(np.array(list(datas[name].U_CPD.values), dtype=float))
    N_data_np = np.stack(N_data)
    N_T_data_train = np.array([N_data_np[:, i:i+T] for i in range(train_days)])
    N_T_data_test = np.array([N_data_np[:, i:i+T] for i in range(train_days, days-T+1)])
    A = [torch.tensor(N_T_data_train[i*batch_size:], dtype=torch.float).to(device) if i == train_days//batch_size else torch.tensor(N_T_data_train[i*batch_size:(i+1)*batch_size], dtype=torch.float).to(device) for i in range(train_days//batch_size+1)]
    B = [torch.tensor(N_T_data_test[i*batch_size:], dtype=torch.float).to(device) if i == test_days//batch_size else torch.tensor(N_T_data_test[i*batch_size:(i+1)*batch_size], dtype=torch.float).to(device) for i in range(test_days//batch_size+1)]
    print("Size of batched model_input U_CPD")
    print("Training set")
    print(f"num of batch : {len(A)}, first batch : {A[0].size()}, last_batch : {A[-1].size()}")
    print("Testing set")
    print(f"num of batch : {len(B)}, first batch : {B[0].size()}, last_batch : {B[-1].size()}")
    return A, B

def Dataloader_Y1(datas, T, batch_size, train_days):
    names = list(datas.keys())
    days = datas["BTCUSDT"].target.values.shape[0]
    test_days = days - train_days - T
    N_data = []
    for name in names:
        N_data.append(np.array(list(datas[name].target.values[T-1:]), dtype=float))
    N_data_np = np.stack(N_data)
    N_T_data_train = np.array([N_data_np[:, i:i+1] for i in range(train_days)])
    N_T_data_test = np.array([N_data_np[:, i:i+1] for i in range(train_days, days-T+1)])
    A = [torch.tensor(N_T_data_train[i*batch_size:], dtype=torch.float).squeeze().to(device) if i == train_days//batch_size else torch.tensor(N_T_data_train[i*batch_size:(i+1)*batch_size], dtype=torch.float).squeeze().to(device) for i in range(train_days//batch_size+1)]
    B = [torch.tensor(N_T_data_test[i*batch_size:], dtype=torch.float).squeeze().to(device) if i == test_days//batch_size else torch.tensor(N_T_data_test[i*batch_size:(i+1)*batch_size], dtype=torch.float).squeeze().to(device) for i in range(test_days//batch_size+1)]
    print("Size of batched target")
    print("Training set")
    print(f"num of batch : {len(A)}, first batch : {A[0].size()}, last_batch : {A[-1].size()}")
    print("Testing set")
    print(f"num of batch : {len(B)}, first batch : {B[0].size()}, last_batch : {B[-1].size()}")
    return A, B

def Dataloader_Y2(datas, T, batch_size, train_days):
    names = list(datas.keys())
    days = datas["BTCUSDT"].target.values.shape[0]
    test_days = days - train_days - T
    N_data = []
    for name in names:
        N_data.append(np.array(list(datas[name].target.values), dtype=float))
    N_data_np = np.stack(N_data)
    N_T_data_train = np.array([N_data_np[:, i:i+T] for i in range(train_days)])
    N_T_data_test = np.array([N_data_np[:, i:i+T] for i in range(train_days, days-T+1)])
    A = [torch.tensor(N_T_data_train[i*batch_size:], dtype=torch.float).to(device) if i == train_days//batch_size else torch.tensor(N_T_data_train[i*batch_size:(i+1)*batch_size], dtype=torch.float).to(device) for i in range(train_days//batch_size+1)]
    B = [torch.tensor(N_T_data_test[i*batch_size:], dtype=torch.float).to(device) if i == test_days//batch_size else torch.tensor(N_T_data_test[i*batch_size:(i+1)*batch_size], dtype=torch.float).to(device) for i in range(test_days//batch_size+1)]
    print("Size of batched target")
    print("Training set")
    print(f"num of batch : {len(A)}, first batch : {A[0].size()}, last_batch : {A[-1].size()}")
    print("Testing set")
    print(f"num of batch : {len(B)}, first batch : {B[0].size()}, last_batch : {B[-1].size()}")
    return A, B

train_days = int(0.8 * data_dic["BTCUSDT"].U.values.shape[0])
test_days = data_dic["BTCUSDT"].U.values.shape[0] - train_days - T
print(train_days)
print(test_days)

# Example usage
batch_size = 32
T = 10

train_loader_X, test_loader_X = Dataloader_X(data_dic, T, batch_size, train_days)
train_loader_Y1, test_loader_Y1 = Dataloader_Y1(data_dic, T, batch_size, train_days)
train_loader_Y2, test_loader_Y2 = Dataloader_Y2(data_dic, T, batch_size, train_days)

# Training loop example
num_epochs = 100
for epoch in range(num_epochs):
    for inputs, targets in zip(train_loader_X, train_loader_Y1):
        optimizer_TFT.zero_grad()
        outputs = TFT_model(inputs, targets)
        loss = criterion(outputs, targets)
        loss.backward()
        optimizer_TFT.step()
    epoch_scheduler_TFT.step()



import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

class GLU(nn.Module):
    def __init__(self, input_dim, output_dim):
        super(GLU, self).__init__()
        self.glu1 = nn.Linear(input_dim, output_dim)
        self.glu2 = nn.Linear(input_dim, output_dim)
        self.sigmoid = nn.Sigmoid()
        
    def forward(self, x):
        return self.sigmoid(self.glu1(x)) * self.glu2(x)

class GRN(nn.Module):
    def __init__(self, input_dim_a, input_dim_c, hidden_dim, output_dim, dropout, additional=False):
        super(GRN, self).__init__()
        self.input_dim_a = input_dim_a
        self.input_dim_c = input_dim_c
        self.hidden_dim = hidden_dim
        self.output_dim = output_dim
        self.additional = additional
        self.trans = nn.Linear(input_dim_a, output_dim)
        self.dense1 = nn.Linear(input_dim_a, hidden_dim)
        self.dense2 = nn.Linear(input_dim_c, hidden_dim)
        self.dense3 = nn.Linear(hidden_dim, hidden_dim)
        self.glu = GLU(hidden_dim, output_dim)
        self.norm = nn.LayerNorm(output_dim)
        self.elu = nn.ELU()
        self.sigmoid = nn.Sigmoid()
        self.dropout = nn.Dropout(dropout)
        
    def forward(self, a, c):
        if self.additional == False:
            c = torch.zeros((a.size(0), self.input_dim_c)).to(a.device)
        eta1 = self.elu(self.dense1(a) + self.dense2(c))
        eta2 = self.dense3(eta1)
        eta2 = self.dropout(eta2)
        eta3 = self.glu(eta2)
        if self.input_dim_a != self.output_dim:
            return self.norm(self.trans(a) + eta3)
        else:
            return self.norm(a + eta3)

class VSN(nn.Module):
    def __init__(self, N, input_dim, input_dim_c, hidden_dim, output_dim, dropout, additional=True):
        super(VSN, self).__init__()
        self.N = N
        self.input_dim = input_dim
        self.input_dim_c = input_dim_c
        self.hidden_dim = hidden_dim
        self.additional = additional
        self.grn_list = nn.ModuleList([GRN(input_dim, 1, hidden_dim, output_dim, dropout, additional=False) for _ in range(N)])
        if additional:
            self.grnfi = GRN(input_dim, input_dim_c, hidden_dim, 1, dropout, additional=True)
        else:
            self.grnfi = GRN(input_dim, 1, hidden_dim, 1, dropout, additional=False)
            
    def forward(self, input, c):
        tilde = [self.grn_list[i](input[i].unsqueeze(0), 0) for i in range(self.N)]
        tilde = torch.stack(tilde, dim=0).squeeze(1)
        v1 = self.grnfi(input, c)
        weight = F.softmax(v1, dim=0)
        output = torch.sum(tilde * weight, dim=0).unsqueeze(0)
        return output

class Static_Encoder(nn.Module):
    def __init__(self, input_dim, hidden_dim, output_dim, dropout):
        super(Static_Encoder, self).__init__()
        self.grn_s = GRN(input_dim, 1, hidden_dim, output_dim, dropout, additional=False)
        self.grn_c = GRN(input_dim, 1, hidden_dim, output_dim, dropout, additional=False)
        self.grn_h = GRN(input_dim, 1, hidden_dim, output_dim, dropout, additional=False)
        self.grn_e = GRN(input_dim, 1, hidden_dim, output_dim, dropout, additional=False)
        
    def forward(self, x):
        c = 0
        Cs = self.grn_s(x, c)
        Cc = self.grn_c(x, c)
        Ch = self.grn_h(x, c)
        Ce = self.grn_e(x, c)
        return Cs, Cc, Ch, Ce

class IMHA(nn.Module):
    def __init__(self, input_dim, hidden_dim, output_dim, num_head):
        super(IMHA, self).__init__()
        self.Wq = nn.Linear(input_dim, hidden_dim)
        self.Wk = nn.Linear(input_dim, hidden_dim)
        self.Wv = nn.Linear(input_dim, hidden_dim)
        self.Wqh_list = nn.ModuleList([nn.Linear(hidden_dim, hidden_dim // num_head) for _ in range(num_head)])
        self.Wkh_list = nn.ModuleList([nn.Linear(hidden_dim, hidden_dim // num_head) for _ in range(num_head)])
        self.WH = nn.Linear(hidden_dim, output_dim)
        self.num_head = num_head
        self.hidden_dim = hidden_dim
        self.head_dim = hidden_dim // num_head
        
    def forward(self, x):
        Q = self.Wq(x)
        K = self.Wk(x)
        V = self.Wv(x)
        sum = torch.zeros((x.size(0), self.hidden_dim)).to(x.device)
        for i in range(self.num_head):
            q = self.Wqh_list[i](Q)
            k = self.Wkh_list[i](K).T
            sum += F.softmax((q @ k) / self.head_dim, dim=1) @ V
        sum = sum / (x.size(0) * self.num_head)
        result = self.WH(sum)
        return result

class TFT(nn.Module):
    def __init__(self, N, T, input_dim, hidden_dim, num_head, dropout):
        super(TFT, self).__init__()
        self.T = T
        self.embedding = nn.Embedding(num_embeddings=N, embedding_dim=hidden_dim)
        self.VSN_ex = VSN(N=N, input_dim=hidden_dim, input_dim_c=1, hidden_dim=hidden_dim, output_dim=hidden_dim, dropout=dropout, additional=False)
        self.static_encoder = Static_Encoder(input_dim=hidden_dim, hidden_dim=hidden_dim, output_dim=hidden_dim, dropout=dropout)
        self.VSN_list = nn.ModuleList([VSN(N=N, input_dim=input_dim, input_dim_c=hidden_dim, hidden_dim=hidden_dim, output_dim=hidden_dim, dropout=dropout, additional=True) for _ in range(T)])
        self.lstm = nn.LSTM(input_size=hidden_dim, hidden_size=hidden_dim, num_layers=1)
        self.imha = IMHA(input_dim=hidden_dim, hidden_dim=hidden_dim, output_dim=hidden_dim, num_head=num_head)
        self.glu1 = GLU(input_dim=hidden_dim, output_dim=hidden_dim)
        self.glu2 = GLU(input_dim=hidden_dim, output_dim=hidden_dim)
        self.glu3 = GLU(input_dim=hidden_dim, output_dim=hidden_dim)
        self.norm = nn.LayerNorm(hidden_dim)
        self.grn1 = GRN(input_dim_a=hidden_dim, input_dim_c=hidden_dim, hidden_dim=hidden_dim, output_dim=hidden_dim, dropout=dropout, additional=True)
        self.grn2 = GRN(input_dim_a=hidden_dim, input_dim_c=1, hidden_dim=hidden_dim, output_dim=hidden_dim, dropout=dropout, additional=False)
        self.dense = nn.Linear(hidden_dim, N)
        self.tanh = nn.Tanh()
        
    def forward(self, input, Sc):
        c = 0
        Sc1 = self.embedding(Sc.to(device))
        Sc2 = self.VSN_ex(Sc1, c)
        Cs, Cc, Ch, Ce = self.static_encoder(Sc2)
        input_vsn = input.transpose(0, 1)
        output_vsn = [self.VSN_list[i](input_vsn[i], Cs) for i in range(self.T)]
        output_vsn = torch.stack(output_vsn, dim=0).squeeze(1)
        lstm_output, _ = self.lstm(output_vsn, (Ch.unsqueeze(0), Cc.unsqueeze(0)))
        glu1_1 = self.glu1(lstm_output) + output_vsn
        glu1_2 = self.norm(glu1_1)
        grn1 = self.grn1(glu1_2, Ce)
        imha_output = self.imha(grn1)
        glu2_1 = self.glu2(imha_output) + grn1
        glu2_2 = self.norm(glu2_1)
        grn2 = self.grn2(glu2_2, c)
        glu3_1 = self.glu3(grn2) + glu1_2
        glu3_2 = self.norm(glu3_1)
        dense_output = self.dense(glu3_2)
        model_output = self.tanh(dense_output)
        return model_output.T

class Sharpe_loss(nn.Module):
    def __init__(self):
        super(Sharpe_loss, self).__init__()
        
    def forward(self, model_output, target):
        mu_R = torch.sum(model_output * target) / (target.numel())
        p = mu_R * torch.sqrt(torch.tensor(365.0))
        q = torch.sqrt(torch.var(model_output * target, dim=None))
        return -p / q

TFT_model = TFT(N=10, T=63, input_dim=2, hidden_dim=64, num_head=4, dropout=0.1).to(device)
criterion = Sharpe_loss().to(device)
optimizer_TFT = torch.optim.Adam(TFT_model.parameters(), lr=0.001)
epoch_scheduler_TFT = torch.optim.lr_scheduler.StepLR(optimizer_TFT, step_size=100, gamma=0.9)

# Example training loop
for epoch in range(num_epochs):
    for inputs, targets in train_loader:
        inputs, targets = inputs.to(device), targets.to(device)
        optimizer_TFT.zero_grad()
        outputs = TFT_model(inputs, targets)
        loss = criterion(outputs, targets)
        loss.backward()
        optimizer_TFT.step()
    epoch_scheduler_TFT.step()

